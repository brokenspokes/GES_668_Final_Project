---
title: "GES668: Property Assessments in Baltimore"
author: "Joshua Spokes"
format: 
  revealjs:
    scrollable: true
    slide-number: true
    multiplex: true
    options:
      autoScale: true
date-modified: 2024-12-14
---


```{r setup, include=FALSE}
library(tidyverse)
library(sf)
library(readr)
library(reactable)
library(ggplot2)
library(knitr)
library(gt)
library(rdeck)
library(tidycensus)
```

## Overview

Are certain types of properties getting a free ride on their taxes?
![Project Origin](Images/2_block_city.png)

::: {.notes}
Back when I was first learning about data analysis, I started a project looking at assessment values in and around the Patterson Bowling Alley. I compared the property taxes being paid by the blocks in the image and found that the commercial property in the middle was paying significantly less for the same amount of land.
:::

## Project goals

1. Identify tranches of property suspected to follow this pattern (Vacant, Auto-Oriented, etc.)

2. Match these patterns to recorded sales

3. Aggregate sales data by block group

![Vacants](Images/vacants.jpeg)

::: {.notes}
My initial goals for the project were to identify sales of property in various categories defined in the proposal, like vacant, unimproved, and auto-oriented. I then planned to compare the sale price to the assessed price for the year and aggregate the data into block groups in order to analyze the sales.

:::

## Changes in Scope

- Analysis was limited to the last five years

- Analysis limited by data quality

- Aggregation switched from census tracts to neighborhoods

::: {.notes}
As I dug into my exploratory analysis, I immediately ran into the limitation that the assessments only include the most recent years. This greatly affected my ability to look at things longitudinally. It was also hard to pluck out sales of homes that were flipped, which resulted in price to assessment ratios that were incredibly high. Finally, I found that it was easier to aggregate sales into neighborhood groups for the purposes of analysis because this provides bigger samples and better opportunities for any kind of insight.
:::


## Data sources

- Maryland CAMA/Assessment data is all public domain

- Baltimore DHCD data licensed under Creative Commons 3.0

- Baltimore Vacants data available under DOI [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.14497481.svg)](https://doi.org/10.5281/zenodo.14497481)

::: {.notes}
The data used for this project come from Maryland's planning cadastre service on IMAP, with additional supporting data coming from Baltimore's open data portal and a dataset of vacant building notices provided by the Baltimore Department of Housing and Community Development. The extracts used for this analysis are all available through Zenodo
:::

## Baltimore Real Property

```{r real-prop, cache=TRUE}
#| label: baltimore-real-prop
st_read("Data/baltimore_real_prop_boundaries.gpkg",
        quiet = TRUE) -> baltimore

baltimore |>
  st_make_valid() |>
  tm_shape() +
  tm_polygons()
```
Baltimore's real property dataset provides additional attributes not provided at the state level like neighborhood and whether it's unimproved property


:::{.notes}
This dataset is available through the Baltimore Open Data portal, so it is easy to download or access with arcgislayers. The extract I am using will be available through this repository. The data is maintained by the city of Baltimore and is created as an extract of maryland's state-level data.
:::

## Baltimore Vacant Building Notices

All vacant building notices issued in Baltimore back to the 1970s
```{r vbns, cache = TRUE}

read_csv("Data/VBNs_All_112624.csv") -> vbns

head(vbns,
     1000) |>
  reactable(defaultPageSize = 10, 
            style = list(fontSize = "12px", maxHeight = "300px"))

```

:::{.notes}
This dataset has been made available via Zenodo: https://doi.org/10.5281/zenodo.14497481. This is created and maintained by the Baltimore Department of Housing and Community Development. It is an important marker because it represents a subset of property being tracked for the purposes of punishing those who do not maintain property.
:::


## Maryland Property Assessment

This data shows the property attributes of every piece of real property in Maryland, which includes location, assessment values, and some building characteristics.
```{r md-real-prop, cache=TRUE}
read.csv("Data/MD_Real_Property_Assessments.csv") -> assessments

head(assessments,
     1000) |>
  reactable(defaultPageSize = 10, 
            style = list(fontSize = "12px", maxHeight = "300px"))

```



## Maryland CAMA Data

Building Characteristics
```{r md-cama, cache = TRUE}
MD_CAMA_Building_Characteristics <- read_csv("Data/MD_CAMA_Building_Characteristics.csv")

head(MD_CAMA_Building_Characteristics,
     1000) |>
  reactable(defaultPageSize = 10, 
            style = list(fontSize = "12px", maxHeight = "200px"))

```

Core

```{r building, CACHE = TRUE}
MD_CAMA_Core <- read_csv("Data/MD_CAMA_Core.csv")

head(MD_CAMA_Core,
     1000) |>
  reactable(defaultPageSize = 10, 
            style = list(fontSize = "12px", maxHeight = "200px"))

```

:::{.notes}
The CAMA, or Computer Assisted Mass Appraisal, data is used to store characteristics of a property for the purposes of generating assessment values at a mass scale. There are two sets used for this project, the Building Characteristics and Core, which provide information on what type of property is present, like residential or commercial. The data used for this project come from Maryland's planning cadastre service on IMAP, with additional supporting data coming from Baltimore's open data portal and a dataset of vacant building notices provided by the Baltimore Department of Housing and Community Development.

:::

## Baltimore Neighborhoods
```{r neighborhoods}
st_read("Data/Neighborhood_Statistical_Area_(NSA)_Boundaries/Neighborhood_Statistical_Area_(NSA)_Boundaries.shp",
        quiet = TRUE) |>
  st_set_geometry("polygon") |>
  st_transform("WGS84") -> neighborhoods


rdeck(
  map_style = mapbox_dark(),
  theme = "kepler",
  initial_bounds = st_bbox(neighborhoods),
  height = 600
  ) |>
  add_polygon_layer(
    name = "Baltimore Neighborhoods",
    data = neighborhoods,
    opacity = 0.6,
    get_fill_color = scale_color_linear(
      Population,
      col_label = "Population"
    ),
    tooltip = c(Name, Population),
    pickable = TRUE
  )
```
:::{.notes}
The Baltimore neighborhoods dataset was created by Baltimore's Department of Planning based on 2020 census data. It provides demographic information for all of the neighborhoods in the city, which means it a convenient place to aggregate

:::


## Analysis Approach

-   Combination of exploratory analysis and visualization

-   Relying heavily on dplyr and sf libraries for analysis

-   rdeck and tmap used for visualization

## Identifying sales of vacant property

```{r approach, eval = FALSE, echo = TRUE}
sales_valid_date |>
  left_join(dates, by = join_by(property == BLOCKLOT)) |>
  mutate(
    vacant_at_sale = (date >= DateNotice) & ((is.na(date_terminate) | date <= date_terminate))
  ) |>
  group_by(BLOCKLOT, sale) |>
  summarise(across(everything(), first),
            vacant_at_sale = any(vacant_at_sale, na.rm = TRUE)
  ) |>
  ungroup() -> vacant_sale
```

I was scratching my head trying to figure this out and asked ChatGPT [@ChatGPT] for help on how to approach this, which resulted in this elegant method for determining where there was a sale of vacant property.

::: {.notes}
My project involved a significant amount of exploratory analysis, looking to determine the best ways to approach the intended goal. As I delved deeper into the project, I found that the quantity of the data or the quality of the data and transformations I was completing makes the comparisons very difficult. The results were not what I was expecting
:::

## Categorizing other sales

```{r categorization, echo = TRUE, eval = FALSE}
left_join(sales, property_land_use, by = join_by(property == BLOCKLOT)) |>
  group_by(property, transfer_no) |>
  summarise(date = first(date),
            price = first(price),
            block = first(block.x),
            property = first(property),
            acct_id_full = first(acct_id_full.x),
            vacant_at_sale = first(vacant_at_sale),
            Land_Value = first(Land_Value),
            Improvement_Value = first(Improvement_Value),
            Total_Assessment = first(Total_Assessment),
            NEIGHBOR = first(NEIGHBOR),
            BL_DSCTYPE = first(BL_DSCTYPE),
            BL_DSCSTYL = first(BL_DSCSTYL),
            CM_DSCIUSE = first(CM_DSCIUSE),
            NO_IMPRV = first(NO_IMPRV),
            .groups = "keep") |>
  mutate(identifier = case_when(
    (NO_IMPRV == "Y" & is.na(BL_DSCTYPE))    ~ "unimproved",
    vacant_at_sale                           ~ "vacant",
    str_detect(BL_DSCTYPE, "AUTO|WAREHOUSE") ~ "unperforming",
    .default = "regular"),
    price_ratio = Total_Assessment / price) |>
  ungroup() -> all_sales
```

:::{.notes}
This chunk shows the characterization of different types of sales based on whether the property was vacant at the time of sale, unimproved, or likely underperforming
:::

## Aggregation by neighborhood

```{r neighborhood-aggregation, echo = TRUE, eval = FALSE}
all_sales |>
  group_by(NEIGHBOR, identifier) |>
  summarise(med_price_ratio = median(price_ratio),
            mean_price_ratio = mean(price_ratio),
            med_price = median(price),
            mean_price = mean(price),
            n = n(),
            .groups = "keep") |>
  pivot_wider(id_cols = "NEIGHBOR",
              names_from = "identifier",
              names_glue = "{identifier}_{.value}",
              values_from = c("med_price_ratio",
                              "mean_price_ratio",
                              "med_price",
                              "mean_price",
                              "n")) %>%
  left_join(neighborhoods, ., by = join_by(Name == NEIGHBOR)) |>
  mutate(pct_blk = Blk_AfAm / Population,
         pct_wht = White / Population) |>
  select(Name,
         Population,
         pct_blk,
         pct_wht,
         starts_with(c("vacant",
                       "unimproved",
                       "unperforming",
                       "regular"))) -> neighborhood_stats
```

## Correlation Testing

The correlation shows a statistically significant (p-value = 0.00139) positive relationship between the median price and median sale to assessment ratio.
```{r cor-test}
readRDS("Data/neighborhood_stats.rds") |>
  st_drop_geometry() -> all_sales

all_sales |>
  filter(regular_n > 50) -> all_sales_over_50

ggplot(all_sales_over_50,
       aes(x = regular_med_price,
             y = regular_med_price_ratio)) +
  geom_point(aes(col = pct_blk)) +
  geom_smooth()

cor(all_sales_over_50$regular_med_price_ratio,
         all_sales_over_50$regular_med_price)
```

:::{.notes}
Correlation testing showed some surprising results, like the fact that the least expensive neighborhoods were actually underassessed, which is the opposite of what we'd expect based on other studies.
:::

## Mapping assessments

Despite what I was expecting, there is no clear pattern here.

```{r med-price-ratio}
st_read("Data/neighborhood_stats.gpkg",
        quiet = TRUE) |>
  filter(regular_n > 50) -> neighborhood_stats

rdeck_dark <- rdeck(map_style = mapbox_dark(),
                    theme = "kepler",
                    initial_bounds = st_bbox(neighborhood_stats),
                    height = 600
)

neighborhood_stats |>
  st_transform(4326) |>
  st_set_geometry("polygon") -> neighborhood_stats

rdeck_dark |>
  add_polygon_layer(
    name = "Assessment Value to Price ratio by Neighborhood",
    data = neighborhood_stats,
    opacity = 0.6,
    get_fill_color = scale_color_linear(
      regular_med_price_ratio,
      col_label = "Price Ratio"
    ),
    tooltip = c(Name, Population, regular_mean_price_ratio, regular_mean_price),
    pickable = TRUE
  ) -> med_assessment_value

med_assessment_value
```

## Difference in price ratio

This map shows neighborhoods where there was at least 25 sales of vacant homes. Based on this, we can see once again that at this level, the assessments subvert hypothesis because vacant price ratios are higher than regular sales.

```{r price-ratio-diff}
st_read("Data/neighborhood_stats.gpkg",
        quiet = TRUE) -> neighborhood_stats

neighborhood_stats |>
  filter(vacant_n > 25) |>
  select(Name,
         Population,
         pct_blk,
         pct_wht,
         vacant_med_price_ratio,
         vacant_mean_price_ratio,
         vacant_mean_price,
         vacant_med_price,
         vacant_n,
         regular_med_price_ratio,
         regular_mean_price_ratio,
         regular_mean_price,
         regular_med_price,
         regular_n) |>
  mutate(diff_mean_price_ratio = regular_mean_price_ratio - vacant_mean_price_ratio,
         diff_med_price_ratio = regular_med_price_ratio - vacant_med_price_ratio) -> vacant_analysis_subset

rdeck_dark <- rdeck(map_style = mapbox_dark(),
                    theme = "kepler",
                    initial_bounds = st_bbox(vacant_analysis_subset),
                    height = 600
)

vacant_analysis_subset |>
  st_transform(4326) |>
  st_set_geometry("polygon") -> vacant_analysis_subset

rdeck_dark |>
  add_polygon_layer(
    name = "Comparative Assessment Value to Price ratio by Neighborhood",
    data = vacant_analysis_subset,
    opacity = 0.6,
    get_fill_color = scale_color_linear(
      diff_med_price_ratio,
      col_label = "Difference in median price ratio"
    ),
    tooltip = c(Name,
                Population,
                regular_med_price_ratio,
                regular_med_price,
                vacant_med_price_ratio,
                vacant_med_price,
                diff_med_price_ratio),
    pickable = TRUE
  ) -> med_diff_assessment_value

med_diff_assessment_value
```

## Challenges in working with data

-   Baltimore City and Maryland State data use separate keys (BLOCKLOT and Account ID)

-   I frequently found myself backtracking to add attributes to various intermediate datasets

-   Getting to the end and realizing that data isn't clean enough for conclusions


## Successes in working with data

I think my analysis does a good job in taking a very big dataset and distilling it to a more useable format. I aimed to make the data more tidy so that it would be easier to create comparisons

```{r dot-density-deck, cache = TRUE}
st_read("neighborhood_stats.gpkg",
        quiet = TRUE) |>
  select(Name,
         ends_with("_n")) |>
  pivot_longer(cols = ends_with("_n"),
               names_to = "Sale_Type",
               values_to = "N_Sales") |>
  mutate(Sale_Type = gsub("_n", "", Sale_Type)) -> pivot

sale_dot_density <- as_dot_density(pivot,
                                   value = "N_Sales",
                                   values_per_dot = 10,
                                   group = "Sale_Type")

rdeck_dark <- rdeck(map_style = mapbox_dark(),
                    theme = "kepler",
                    initial_bounds = st_bbox(pivot),
                    height = 600
)

sale_dot_density |>
  st_transform(4326) |>
  st_set_geometry("position") -> sale_dot_density

rdeck_dark |>
  add_scatterplot_layer(
    name = "Sales by type of property (10 per dot)",
    data = sale_dot_density,
    radius_min_pixels = 1,
    radius_max_pixels = 5,
    radius_scale = 5,
    opacity = 0.6,
    get_fill_color = scale_color_category(
      Sale_Type,
      palette = scales::brewer_pal("qual"),
      col_label = "Sale Type"
    )
  )
```
## Where to learn more

::: {.r-fit-text}

All data, code, an explanation of the analysis and inspirations are available through the README on my [project repository](https://github.com/bldgspatialdata/final-project-brokenspokes)

:::
